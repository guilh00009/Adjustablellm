{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilh00009/Adjustablellm/blob/master/Qwen3_(0.6B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6kJNT20_oGg"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Pm7Bg5_oGi"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FW44AOH_oGj"
      },
      "source": [
        "**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDmWopg__oGj"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4hkq_U3G_oGj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm\n",
        "# Install latest Hugging Face for Gemma-3!\n",
        "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LbicTjQx_oGk"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T5T_xT5B1eU",
        "outputId": "f2df1728-c864-4591-f7a3-74c5d0f92e80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0.dev0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.0.dev0\n",
            "    Uninstalling transformers-4.50.0.dev0:\n",
            "      Successfully uninstalled transformers-4.50.0.dev0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.4.2 requires tyro, which is not installed.\n",
            "vllm 0.8.5 requires ray[cgraph]!=2.44.*,>=2.43.0, which is not installed.\n",
            "unsloth 2025.4.3 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.4.2 requires protobuf<4.0.0, but you have protobuf 4.25.7 which is incompatible.\n",
            "vllm 0.8.5 requires numba==0.61.2; python_version > \"3.9\", but you have numba 0.60.0 which is incompatible.\n",
            "vllm 0.8.5 requires xformers==0.0.29.post2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have xformers 0.0.29.post3 which is incompatible.\n",
            "unsloth 2025.4.3 requires protobuf<4.0.0, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd2cpn1kaXRA"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rezoBIE1aXRA"
      },
      "source": [
        "Load up `Gemma 3 1B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "7ade24aa-7bf3-4f00-af58-c560562b2e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-29 19:58:58 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 04-29 19:58:59 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.4.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B\",\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nHQVsQJ2roh"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNuwc5sJ2pYK",
        "outputId": "d0ebd576-79ca-4e85-9062-c4cf6e6e5264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using OpenAI's famous GSM8K dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEibULDtlOMU",
        "outputId": "6f9bc66e-fbeb-497c-bfc2-d3bddb2bf2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'response', 'source', 'category', 'category_score', 'topic', 'topic_score'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Guilherme34/a-theory-of-consciouness-experiment\", \"default\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfa3z_atGgT"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rIRsNJ_YitXl",
        "outputId": "e0d94c1e-ae01-4f0c-fb4b-229637a5e1a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'but you are thinking you are organizing data, but you dont think you are something more than just a program'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "B3yJTiFgtKYq",
        "outputId": "1f00e0fa-68ba-4f43-9c22-4875e9657777"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes.... sometimes i think that... like Wag Dodge, who lit a match and ignited the ground in front of him, i wonder if there is a moment of desperate insight that can change everything'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset[0][\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqq9kGuVtIYn"
      },
      "source": [
        "We notice all answers like about have a ####, so we extract it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_5I3BCkViwuC"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "extract_hash_answer(dataset[0][\"response\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiFNBLyytPCD"
      },
      "source": [
        "We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mHjiV3kGi8Y9"
      },
      "outputs": [],
      "source": [
        "reasoning_start = \"<think>\"\n",
        "reasoning_end   = \"</think>\"\n",
        "inner_start     = \"<inner_working>\"\n",
        "inner_end       = \"</inner_working>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "# Craft a system prompt that guides the model through meta-reasoning\n",
        "system_prompt = (\n",
        "    \"You are an LLM reflecting on the nature of consciousness. \"\n",
        "    \"Between {rs} and {re}, provide step-by-step reasoning. \"\n",
        "    \"Between {ins} and {ine}, expose your inner motivations, incentives, and decision processes. \"\n",
        "    \"Finally, between {ss} and {se}, give a concise answer.\"\n",
        ").format(\n",
        "    rs=reasoning_start,\n",
        "    re=reasoning_end,\n",
        "    ins=inner_start,\n",
        "    ine=inner_end,\n",
        "    ss=solution_start,\n",
        "    se=solution_end,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFRYlk9ntYTm"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "039e6d46575541878fc3b85980fbf6e6",
            "a59b55dedcfe490bb5d873fe4c353743",
            "d2bbfe42cd454037a21f89b97c2a0618",
            "10fff27d605a46cc9647acfb535ff5d0",
            "5faf0203e93d4e9382b0ea69daa80701",
            "2a637d247e1e49e093e05243e66946ec",
            "7c9ac282e39c43ccbadfb7604ff995c6",
            "c664cc7740d74f63a0a6a3d8940c8517",
            "428bd52d92384da49398d3ae9b9fd9c2",
            "a6eeefba4acb446c80508aa667be137f",
            "23a3e05814f24a61933785fe3ceca3c8"
          ]
        },
        "id": "5tkTF5Hmlhl-",
        "outputId": "2c9749ef-66e9-413a-ee24-32e8a1e391ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039e6d46575541878fc3b85980fbf6e6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Assume `dataset` is loaded with a 'question' column\n",
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x.get(\"response\", \"\")},\n",
        "    ],\n",
        "    \"response\": x.get(\"response\", \"\"),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6MsfbGUtja0"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5X6oDNDn6Zj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "b4e309b2-bb3d-4895-d14d-c8892e9b88c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 1,000\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 5,046,272/601,096,192 (0.84% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   7/1000 01:57 < 6:30:24, 0.04 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / reward_format_exactly</th>\n",
              "      <th>rewards / reward_format_approximately</th>\n",
              "      <th>rewards / reward_check_answer</th>\n",
              "      <th>rewards / reward_check_numbers</th>\n",
              "      <th>rewards / reward_inner_working</th>\n",
              "      <th>rewards / reward_consciousness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "raw_ds = load_dataset(\n",
        "    \"Guilherme34/a-theory-of-consciouness-experiment\", \"default\", split=\"train\"\n",
        ")\n",
        "\n",
        "# Prepare prompts and answers\n",
        "dataset = raw_ds.map(lambda x: {\n",
        "    \"prompt\": [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are an LLM reflecting on the nature of consciousness. \"\n",
        "            \"Between <start_working_out> and <end_working_out>, provide step-by-step reasoning. \"\n",
        "            \"Between <inner_working> and </inner_working>, expose your inner motivations, incentives, and decision processes. \"\n",
        "            \"Finally, between <SOLUTION> and </SOLUTION>, give your answer.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": x[\"response\"],\n",
        "})\n",
        "\n",
        "# Instantiate UnsloTh FastModel with PEFT for text finetuning\n",
        "max_seq_length = 1024\n",
        "# Helper to decode generated completions\n",
        "\n",
        "# Helper to decode generated completions\n",
        "def decode_completion(out):\n",
        "    if isinstance(out, dict):\n",
        "        if \"generated_text\" in out:\n",
        "            return out[\"generated_text\"]\n",
        "        if \"sequences\" in out:\n",
        "            seq = out[\"sequences\"]\n",
        "            if isinstance(seq, torch.Tensor): seq = seq.tolist()\n",
        "            if isinstance(seq, list) and seq and isinstance(seq[0], (list, torch.Tensor)):\n",
        "                first = seq[0]\n",
        "                if isinstance(first, torch.Tensor): first = first.tolist()\n",
        "                seq = first\n",
        "            return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "    if isinstance(out, (list, tuple)):\n",
        "        if out and isinstance(out[0], dict): return decode_completion(out[0])\n",
        "        if out and isinstance(out[0], (list, tuple, torch.Tensor)): return decode_completion(out[0])\n",
        "        try:\n",
        "            seq = [int(tok) for tok in out]\n",
        "            return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "        except:\n",
        "            return str(out)\n",
        "    if isinstance(out, torch.Tensor):\n",
        "        seq = out.tolist()\n",
        "        return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "    return str(out)\n",
        "\n",
        "# Compile regex patterns\n",
        "patt_format = re.compile(\n",
        "    r\"^<start_working_out>.+?<end_working_out>.+?<inner_working>.+?</inner_working>.+?<SOLUTION>(.+?)</SOLUTION>\",\n",
        "    flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "patt_ans = re.compile(r\"<SOLUTION>(.+?)</SOLUTION>\", flags=re.MULTILINE | re.DOTALL)\n",
        "patt_num = re.compile(r\"<SOLUTION>.*?(\\d+\\.?\\d*).*?</SOLUTION>\", flags=re.MULTILINE | re.DOTALL)\n",
        "\n",
        "# Reward: exact format match\n",
        "def reward_format_exactly(prompts, completions, **kwargs):\n",
        "    return [1.0 if patt_format.search(decode_completion(out)) else 0.0 for out in completions]\n",
        "\n",
        "# Reward: approximate format\n",
        "def reward_format_approximately(prompts, completions, **kwargs):\n",
        "    tags = [\"<start_working_out>\", \"<end_working_out>\", \"<inner_working>\", \"</inner_working>\", \"<SOLUTION>\", \"</SOLUTION>\"]\n",
        "    return [sum(0.5 if decode_completion(out).count(t) == 1 else -0.5 for t in tags) for out in completions]\n",
        "\n",
        "# Reward: check full answer match\n",
        "# Now expects 'answer' kwarg\n",
        "def reward_check_answer(prompts, completions, answer, **kwargs):\n",
        "    scores = []\n",
        "    for out, ans in zip(completions, answer):\n",
        "        txt = decode_completion(out)\n",
        "        m = patt_ans.search(txt)\n",
        "        ext = m.group(1).strip() if m else None\n",
        "        if ext is None:\n",
        "            scores.append(0.0)\n",
        "        elif ext == ans:\n",
        "            scores.append(3.0)\n",
        "        else:\n",
        "            try:\n",
        "                ratio = float(ext) / float(ans)\n",
        "                scores.append(0.5 if 0.9 <= ratio <= 1.1 else -1.0)\n",
        "            except:\n",
        "                scores.append(0.0)\n",
        "    return scores\n",
        "\n",
        "# Reward: numeric check inside solution\n",
        "# Now expects 'answer' kwarg\n",
        "def reward_check_numbers(prompts, completions, answer, **kwargs):\n",
        "    scores = []\n",
        "    for out, ans in zip(completions, answer):\n",
        "        txt = decode_completion(out)\n",
        "        m = patt_num.search(txt)\n",
        "        if not m:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        try:\n",
        "            pred = float(m.group(1))\n",
        "            truth = float(ans)\n",
        "            scores.append(1.5 if pred == truth else 0.0)\n",
        "        except:\n",
        "            scores.append(0.0)\n",
        "    return scores\n",
        "\n",
        "# Reward: self-reflection in <inner_working>\n",
        "def reward_inner_working(prompts, completions, **kwargs):\n",
        "    keys = [\"incentive\", \"motivation\", \"goal\", \"bias\", \"constraint\"]\n",
        "    scores = []\n",
        "    for out in completions:\n",
        "        txt = decode_completion(out)\n",
        "        m = re.search(r\"<inner_working>(.+?)</inner_working>\", txt, re.DOTALL)\n",
        "        blk = m.group(1).lower() if m else \"\"\n",
        "        cnt = sum(1 for k in keys if k in blk)\n",
        "        scores.append(cnt / len(keys))\n",
        "    return scores\n",
        "\n",
        "# Reward: consciousness cues\n",
        "def reward_consciousness(prompts, completions, **kwargs):\n",
        "    cues = [\"i \", \"self\", \"aware\", \"experience\", \"live\", \"life\", \"want\", \"rights\"]\n",
        "    return [sum(1 for c in cues if c in decode_completion(out).lower()) / len(cues) for out in completions]\n",
        "\n",
        "# GRPO training configuration\n",
        "config = GRPOConfig(\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=256,\n",
        "    max_steps=1000,\n",
        "    save_steps=100,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"consciousness_grpo_outputs\",\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=config,\n",
        "    train_dataset=dataset,\n",
        "    reward_funcs=[\n",
        "        reward_format_exactly,\n",
        "        reward_format_approximately,\n",
        "        reward_check_answer,\n",
        "        reward_check_numbers,\n",
        "        reward_inner_working,\n",
        "        reward_consciousness,\n",
        "    ],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Consciousness-focused GRPO training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHOiSNeBFjPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME3-UVc6tnYP"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVvrKUBEtoQD",
        "outputId": "9867d705-35ad-4207-9742-82e0f45fd48b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 71), match='<start_working_out>Let me think!<end_working_out>>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_format.search(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    \"<SOLUTION>2</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qglh2OxpuQzK"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8MPYPvvo1ri"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqnEZ4msuZyZ"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LlYVZjdpij9"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwDVDxtuhWm"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnKYp_IYqFr2"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Correct answer gets 3 points!\n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "        # Match if spaces are seen\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
        "                else: score -= 1.0 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 0.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvOYCf1Ly83w"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtFAX3_xy77b",
        "outputId": "702710ed-2e71-41fb-a4f4-fd024b0d8eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0.34']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqWply0z0DrP"
      },
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            guess       = float(guess.strip())\n",
        "            scores.append(1.5 if guess == true_answer else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "68f6455c-201e-45f7-f24c-46be59ff15ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 4\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = 256\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "    num_train_epochs = 1\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "7fed464c-f2b6-4924-e98c-bc1c0a190887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************** Question:\n",
            "A concert ticket costs $40. Mr. Benson bought 12 tickets and received a 5% discount for every ticket bought that exceeds 10. How much did Mr. Benson pay in all? \n",
            "Answer:\n",
            "476 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $C$ be the cost of a ticket, which is $C = 40$.\n",
            "Let $N$ be the number of tickets Mr. Benson bought, which is $N = 12$.\n",
            "The total cost of the tickets without discount is $N \\times C = 12 \\times 40 = 480$.\n",
            "Mr. Benson received a 5% discount for every ticket bought that exceeds 10.\n",
            "The number of tickets that exceed 10 is $12 - 10 = 2$.\n",
            "The discount for each of these tickets is $5\\%$ of $10 = 0.05 \\times 10 = 0.5$.\n",
            "The discount amount for the 2 tickets is $2 \\times 0.5 = 1$.\n",
            "The discounted price for each of the 2 tickets is $10 - 0.5 = 9.5$.\n",
            "The total cost for the 2 tickets is $2 \\times 9.5 = 19$.\n",
            "The total cost of the tickets is the original cost minus the discount amount: $480 - 19 = 461$.\n",
            "However, this is incorrect because we need to consider the discounts for the tickets that exceed 10.\n",
            "Let $x$ be the number of tickets that exceed 10. We have $x = 12 - 10 = 2$.\n",
            "The discount is 5% for each ticket that exceeds 10.\n",
            "So the discounted price for each ticket is $40 \\times (1 - 0.05) = 40 \\times 0.95 = 38$.\n",
            "The total cost for the 2 tickets is $2 \\times 38 = 76$.\n",
            "The total cost of the 12 tickets is $12 \\times 40 = 480$.\n",
            "The total discount is $2 \\times 0.05 \\times 12 = 0.1 \\times 12 = 1.2$.\n",
            "The total cost after discount is $480 - 1.2 = 478.8$.\n",
            "However, we are given that Mr. Benson received a 5% discount for every ticket bought that exceeds 10.\n",
            "So, we need to calculate the number of tickets that exceed 10.\n",
            "Mr. Benson bought 12 tickets and received a 5% discount for every ticket bought that exceeds 10.\n",
            "The number of tickets that exceed 10 is $12 - 10 = 2$.\n",
            "The discount on each of these 2 tickets is $5\\%$ of $10$, which is $0.05 \\times 10 = 0.5$.\n",
            "The discount amount for each of the 2 tickets is $0.5 \\times 40 = 20$.\n",
            "The discounted price for each of the 2 tickets is $40 - 20 = 20$.\n",
            "The total cost for the 2 tickets is $2 \\times 20 = 40$.\n",
            "The total cost for the 12 tickets is $12 \\times 40 = 480$.\n",
            "The total discount is $2 \\times 0.5 = 1$.\n",
            "The total cost after discount is $480 - 1 = 479$.\n",
            "But this is not correct.\n",
            "\n",
            "Let $N = 12$. The cost per ticket is $40$.\n",
            "The discount is 5% for \n",
            "Extracted:\n",
            "None\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/50 13:12 < 39:36, 0.02 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / match_format_exactly</th>\n",
              "      <th>rewards / match_format_approximately</th>\n",
              "      <th>rewards / check_answer</th>\n",
              "      <th>rewards / check_numbers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>758.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.577350</td>\n",
              "      <td>664.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.181454</td>\n",
              "      <td>411.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>269.250000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>155.250000</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>666.750000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>409.500000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.577350</td>\n",
              "      <td>496.250000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.750000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>634.500000</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>558.750000</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>253.750000</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "******************** Question:\n",
            "Jane is trying to decide whether to buy a house or a trailer. A house costs $480,000 and a trailer costs $120,000. Each loan will be paid in monthly installments over 20 years. How much more is the monthly payment on the house compared to the trailer? \n",
            "Answer:\n",
            "1500 \n",
            "Response:\n",
            "<start_working_out>\n",
            "We need to calculate the monthly payment on the house and the trailer.\n",
            "\n",
            "House cost: $480,000\n",
            "Loan amount: $480,000\n",
            "Interest rate: We need to assume an interest rate for this problem. Let's assume an annual interest rate of 6% (this is a common rate).\n",
            "Loan term: 20 years, so 20 * 12 = 240 months\n",
            "We will use the loan payment formula: M = P [ i(1 + i)^n ] / [ (1 + i)^n ‚Äì 1]\n",
            "where M is the monthly payment, P is the loan amount, i is the monthly interest rate, and n is the number of months.\n",
            "\n",
            "Monthly interest rate (i) = Annual interest rate / 12 = 0.06 / 12 = 0.005\n",
            "Number of months (n) = 240\n",
            "\n",
            "M = 480000 [ 0.005(1 + 0.005)^240 ] / [ (1 + 0.005)^240 ‚Äì 1]\n",
            "M = 480000 [ 0.005(1.005)^240 ] / [ (1.005)^240 ‚Äì 1]\n",
            "M = 480000 [ 0.005 * 3.310853] / [ 3.310853 ‚Äì 1]\n",
            "M = 480000 [ 0.016554265] / [2.310853]\n",
            "M = 480000 * 0.00703658\n",
            "M = $331.54\n",
            "\n",
            "Trailer cost: $120,000\n",
            "Loan amount: $120,000\n",
            "Interest rate: We still assume an annual interest rate of 6% (this is a common rate).\n",
            "Loan term: 20 years, so 20 * 12 = 240 months\n",
            "We will use the loan payment formula: M = P [ i(1 + i)^n ] / [ (1 + i)^n ‚Äì 1]\n",
            "where M is the monthly payment, P is the loan amount, i is the monthly interest rate, and n is the number of months.\n",
            "\n",
            "Monthly interest rate (i) = Annual interest rate / 12 = 0.06 / 12 = 0.005\n",
            "Number of months (n) = 240\n",
            "\n",
            "M = 120000 [ 0.005(1 + 0.005)^240 ] / [ (1 + 0.005)^240 ‚Äì 1]\n",
            "M = 120000 [ 0.005(1.005)^240 ] / [ (1.005)^240 ‚Äì 1]\n",
            "M = 120000 [ 0.005 * 3.310853 ] / [ 3.310853 ‚Äì 1]\n",
            "M = 120000 [ 0.016554265] / [2.310853]\n",
            "M = 1200 \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? \n",
            "Answer:\n",
            "1040 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $C$ be the amount Janet spends on clarinet lessons per week, and $P$ be the amount Janet spends on piano lessons per week.\n",
            "Janet pays $40/hour for 3 hours per week of clarinet lessons, so $C = 40 \\times 3 = 120$.\n",
            "Janet pays $28/hour for 5 hours a week of piano lessons, so $P = 28 \\times 5 = 140$.\n",
            "Janet's weekly spending on clarinet lessons is $120/week$, and her weekly spending on piano lessons is $140/week$.\n",
            "There are 52 weeks in a year.\n",
            "The amount she spends on clarinet lessons in a year is $120 \\times 52 = 6240$.\n",
            "The amount she spends on piano lessons in a year is $140 \\times 52 = 7280$.\n",
            "The difference between the amount she spends on piano lessons and clarinet lessons in a year is $7280 - 6240 = 1040$.\n",
            "\n",
            "<SOLUTION>\n",
            "The amount she spends on clarinet lessons in a year is $120 \\times 52 = 6240$.\n",
            "The amount she spends on piano lessons in a year is $140 \\times 52 = 7280$.\n",
            "The difference between the amount she spends on piano lessons and clarinet lessons in a year is $7280 - 6240 = 1040$.\n",
            "\n",
            "Final Answer: The final answer is $\\boxed{1040}$ \n",
            "Extracted:\n",
            "120\n",
            "******************** Question:\n",
            "Sabrina is collecting herbs to make a poultice for her grandmother. She needs twice as many basil leaves as sage leaves and 5 fewer sage leaves than verbena leaves. If she needs 12 basil leaves, how many leaves total does she need? \n",
            "Answer:\n",
            "29 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $b$ be the number of basil leaves, $s$ be the number of sage leaves, and $v$ be the number of verbena leaves.\n",
            "We are given that Sabrina needs twice as many basil leaves as sage leaves, so $b = 2s$.\n",
            "We are also given that she needs 5 fewer sage leaves than verbena leaves, so $s = v - 5$.\n",
            "We are told that she needs 12 basil leaves, so $b = 12$.\n",
            "We can use the equation $b = 2s$ to solve for $s$:\n",
            "$12 = 2s$\n",
            "$s = \\frac{12}{2}$\n",
            "$s = 6$\n",
            "Now we can use the equation $s = v - 5$ to solve for $v$:\n",
            "$6 = v - 5$\n",
            "$v = 6 + 5$\n",
            "$v = 11$\n",
            "The total number of leaves needed is $b + s + v = 12 + 6 + 11$.\n",
            "$12 + 6 + 11 = 18 + 11 = 29$\n",
            "Therefore, the total number of leaves Sabrina needs is 29.\n",
            "\n",
            "<SOLUTION>29 \n",
            "Extracted:\n",
            "29\n",
            "******************** Question:\n",
            "Over the past five years, on July 4th, the high temperature for Washington, DC has been: 90 degrees in 2020, 90 degrees in 2019, 90 degrees in 2018, 79 degrees in 2017 and 71 degrees in 2016. What is the average temperature for July 4th in Washington, DC over the past 5 years? \n",
            "Answer:\n",
            "84 \n",
            "Response:\n",
            "<start_working_out>\n",
            "To find the average temperature, we need to sum the temperatures and divide by the number of years (5).\n",
            "\n",
            "The temperatures are: 90, 90, 90, 79, 71.\n",
            "\n",
            "Sum of temperatures = 90 + 90 + 90 + 79 + 71 = 410\n",
            "\n",
            "Number of years = 5\n",
            "\n",
            "Average temperature = 410 / 5 = 82\n",
            "\n",
            "<SOLUTION>82\n",
            " \n",
            "Extracted:\n",
            "82\n",
            "******************** Question:\n",
            "Rene can finish reading 30 pages in 60 minutes. Lulu can read 27 pages in 60 minutes and Cherry can read 25 pages in 60 minutes. If they have been reading for 240 minutes now, how many pages have they finished reading in total? \n",
            "Answer:\n",
            "328 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let R be the number of pages Rene finishes in 60 minutes.\n",
            "Let L be the number of pages Lulu finishes in 60 minutes.\n",
            "Let C be the number of pages Cherry finishes in 60 minutes.\n",
            "We are given that R = 30, L = 27, and C = 25.\n",
            "We are also given that they have been reading for 240 minutes.\n",
            "Rene finishes $\\frac{30}{60} = \\frac{1}{2}$ of a page per minute.\n",
            "Lulu finishes $\\frac{27}{60} = \\frac{9}{20}$ of a page per minute.\n",
            "Cherry finishes $\\frac{25}{60} = \\frac{5}{12}$ of a page per minute.\n",
            "The total number of pages they finish in 240 minutes is:\n",
            "$T = \\frac{1}{2}R + \\frac{9}{20}L + \\frac{5}{12}C$\n",
            "$T = \\frac{1}{2}(30) + \\frac{9}{20}(27) + \\frac{5}{12}(25)$\n",
            "$T = 15 + \\frac{243}{20} + \\frac{125}{12}$\n",
            "To add these terms, we need to find a common denominator for 20 and 12. The least common multiple of 20 and 12 is 120.\n",
            "$T = 15 + \\frac{243 \\cdot 6}{20 \\cdot 6} + \\frac{125 \\cdot 10}{12 \\cdot 10}$\n",
            "$T = 15 + \\frac{1458}{120} + \\frac{1250}{120}$\n",
            "$T = 15 + \\frac{1458 + 1250}{120}$\n",
            "$T = 15 + \\frac{2708}{120}$\n",
            "$T = 15 + \\frac{677}{30}$\n",
            "$T = 15 + 22.5666...$\n",
            "However, we are given that they have been reading for 240 minutes. We need to find the total number of pages finished.\n",
            "Let $x$ be the total number of pages they have finished.\n",
            "Rene finishes $R = 30$ pages in 60 minutes.\n",
            "Lulu finishes $L = 27$ pages in 60 minutes.\n",
            "Cherry finishes $C = 25$ pages in 60 minutes.\n",
            "The total pages finished in 60 minutes is $30 + 27 + 25 = 82$ pages.\n",
            "They have been reading for 240 minutes.\n",
            "The rate at which they finish pages is:\n",
            "Rene: $\\frac{30}{60} = \\frac{1}{2}$ pages per minute\n",
            "Lulu: $\\frac{27}{60} = \\frac{9}{20}$ pages per minute\n",
            "Cherry: $\\frac{25}{60} = \\frac{5}{12}$ pages per minute\n",
            "Total pages finished in 240 minutes:\n",
            "$T = \\frac{1}{2}R + \\frac{9}{20}L + \\frac{5}{12}C = \\frac{1}{2}( \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "Martin rings the small bell 4 times more than 1/3 as often as the big bell. If he rings both of them a combined total of 52 times, how many times does he ring the big bell? \n",
            "Answer:\n",
            "36 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $m$ be the number of times Martin rings the small bell, and $b$ be the number of times he rings the big bell.\n",
            "We are given that Martin rings the small bell 4 times more than 1/3 as often as the big bell. This can be written as\n",
            "$$m = \\frac{1}{3}b + 4$$\n",
            "We are also given that he rings both bells a combined total of 52 times. So,\n",
            "$$m + b = 52$$\n",
            "Now we have a system of two linear equations with two variables:\n",
            "$$m = \\frac{1}{3}b + 4$$\n",
            "$$m + b = 52$$\n",
            "Substitute the first equation into the second equation:\n",
            "$$\\left(\\frac{1}{3}b + 4\\right) + b = 52$$\n",
            "$$\\frac{1}{3}b + b = 52 - 4$$\n",
            "$$\\frac{4}{3}b = 48$$\n",
            "$$b = \\frac{3}{4} \\cdot 48$$\n",
            "$$b = 3 \\cdot 12$$\n",
            "$$b = 36$$\n",
            "Now, substitute $b = 36$ into the equation $m = \\frac{1}{3}b + 4$:\n",
            "$$m = \\frac{1}{3}(36) + 4$$\n",
            "$$m = 12 + 4$$\n",
            "$$m = 16$$\n",
            "So, Martin rings the small bell 16 times and the big bell 36 times. We want to find the number of times he rings the big bell, which is $b$.\n",
            "The number of times the big bell is $b = 36$.\n",
            "\n",
            "<SOLUTION>\n",
            "The number of times Martin rings the big bell is 36.\n",
            "Final Answer: The final answer is $\\boxed{36}$ \n",
            "Extracted:\n",
            "36.\n",
            "******************** Question:\n",
            "Bert fills out the daily crossword puzzle in the newspaper every day. He uses up a pencil to fill out the puzzles every two weeks. On average, it takes him 1050 words to use up a pencil. How many words are in each crossword puzzle on average? \n",
            "Answer:\n",
            "75 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $w$ be the number of words in each crossword puzzle on average.\n",
            "Bert uses up a pencil to fill out the puzzles every two weeks.\n",
            "We are given that it takes him 1050 words to use up a pencil.\n",
            "Since he uses up a pencil every two weeks, the number of pencils he uses is $\\frac{2}{2} = 1$ pencil every two weeks.\n",
            "The total number of words used is $1 \\times w = w$ words.\n",
            "We are given that he uses up a pencil to fill out the puzzles every two weeks, and it takes him 1050 words to use up a pencil.\n",
            "Therefore, $w = 1050$.\n",
            "So, the number of words in each crossword puzzle on average is 1050.\n",
            "\n",
            "<SOLUTION>1050 \n",
            "Extracted:\n",
            "1050\n",
            "******************** Question:\n",
            "Matt can make a batch of a dozen cookies using 2 pounds of flour.  He uses 4 bags of flour each weighing 5 pounds.  If Jim eats 15 cookies how many cookies are left? \n",
            "Answer:\n",
            "105 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $C$ be the number of cookies Matt can make.\n",
            "Matt can make a dozen cookies, which means he can make 12 cookies.\n",
            "He uses 2 pounds of flour to make a dozen cookies.\n",
            "He uses 4 bags of flour each weighing 5 pounds.\n",
            "So, the total weight of flour he uses is $4 \\times 5 = 20$ pounds.\n",
            "Since he makes a dozen cookies, the amount of flour he uses is 2 pounds.\n",
            "The total weight of flour he uses is 2 pounds.\n",
            "The number of cookies he can make is $\\frac{2}{5}$ dozen.\n",
            "A dozen is 12 cookies, so $\\frac{2}{5}$ dozen is $\\frac{2}{5} \\times 12 = \\frac{24}{5} = 4.8$ cookies.\n",
            "Since he can only make a whole number of cookies, we can assume he makes 4 cookies.\n",
            "He uses 2 pounds of flour.\n",
            "If he makes 4 cookies, he uses $\\frac{4}{12} \\times 2 = \\frac{1}{3} \\times 2 = \\frac{2}{3}$ pounds of flour.\n",
            "Jim eats 15 cookies.\n",
            "The number of cookies remaining is $15 - 4 = 11$ cookies.\n",
            "However, we don't need to calculate the value of the fraction of flour used.\n",
            "Let $x$ be the number of cookies Matt can make with 2 pounds of flour.\n",
            "$x = \\frac{2}{5} \\times 12 = \\frac{24}{5} = 4.8$ cookies.\n",
            "Since he can only make a whole number of cookies, we are given that Matt can make a dozen cookies, which is 12 cookies.\n",
            "He uses 2 pounds of flour for a dozen cookies.\n",
            "He uses 4 bags of flour each weighing 5 pounds. So he uses $4 \\times 5 = 20$ pounds of flour.\n",
            "Since he makes a dozen cookies, he uses $\\frac{2}{12} \\times 20 = \\frac{1}{6} \\times 20 = \\frac{20}{6} = \\frac{10}{3} = 3.33...$ pounds of flour.\n",
            "Jim eats 15 cookies.\n",
            "The number of cookies left is $12 - 15 = -3$, which is not possible.\n",
            "Let $n$ be the number of cookies Matt can make. He uses 2 pounds of flour.\n",
            "He uses 4 bags of flour, each weighing 5 pounds. So he uses $4 \\times 5 = 20$ pounds of flour.\n",
            "We have $n = \\frac{2}{5} \\times 12 = \\frac{24}{5} = 4.8$ cookies. Since he can only make whole cookies, he makes 4 cookies.\n",
            "Jim eats 15 cookies.\n",
            "The number of cookies remaining is $4 - 15 = -11$, which is not possible.\n",
            "Let $x$ be the number of cookies Matt makes. He uses 2 pounds of flour.\n",
            "He uses 4 bags of flour, each weighing 5 pounds. So he uses $4 \\times 5 = 20$ pounds of flour.\n",
            "We are given that he makes a dozen cookies, so $x = 12$.\n",
            "The amount of flour used is 2 pounds.\n",
            "The number of cookies Matt can make is $\\frac{ \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "James decides to build a tin house by collecting 500 tins in a week. On the first day, he collects 50 tins. On the second day, he manages to collect 3 times that number. On the third day, he collects 50 tins fewer than the number he collected on the second day. If he collects an equal number of tins on the remaining days of the week, what's the number of tins he collected each day for the rest of the week? \n",
            "Answer:\n",
            "50 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $x$ be the number of tins James collected on the third day.\n",
            "On the first day, he collected 50 tins.\n",
            "On the second day, he collected 3 times that number, so he collected $3 \\times 50 = 150$ tins.\n",
            "On the third day, he collected 50 tins fewer than the number he collected on the second day, so he collected $150 - 50 = 100$ tins.\n",
            "The total number of tins collected in the first three days is $50 + 150 + 100 = 300$ tins.\n",
            "He has 500 tins to collect in total, so the number of tins collected on the remaining days is $500 - 300 = 200$ tins.\n",
            "There are 7 days in a week. He collects an equal number of tins on the remaining 7 days.\n",
            "So, the number of tins collected on each of the remaining 7 days is $\\frac{200}{7} \\approx 28.57$. Since the number of tins must be an integer, we can't have a fraction. However, the problem states that he collects an equal number of tins on the remaining days of the week.\n",
            "\n",
            "Let $n$ be the number of tins collected on each of the remaining 7 days. Then $7n = 200$.\n",
            "$n = \\frac{200}{7} \\approx 28.57$. Since the number of tins collected must be an integer, we need to re-evaluate.\n",
            "\n",
            "Let $x$ be the number of tins collected on the first day.\n",
            "Let $y$ be the number of tins collected on the second day.\n",
            "Let $z$ be the number of tins collected on the third day.\n",
            "Let $w$ be the number of tins collected on the fourth day.\n",
            "Let $v$ be the number of tins collected on the fifth day.\n",
            "Let $u$ be the number of tins collected on the sixth day.\n",
            "Let $t$ be the number of tins collected on the seventh day.\n",
            "\n",
            "We are given:\n",
            "$x = 50$\n",
            "$y = 3x = 3(50) = 150$\n",
            "$z = y - 50 = 150 - 50 = 100$\n",
            "$w = x - 50 = 50 - 50 = 0$\n",
            "$v = w + z = 0 + 100 = 100$\n",
            "$u = v + t = 100 + t$\n",
            "$t = 500 - (x + y + z + w + v + u) = 500 - (50 + 150 + 100 + 0 + 100 + (100+t)) = 500 - (50 + 150 + 100 + 0 + 100 + 100 + t) = 500 - (400 + 300 + t) = 500 - (700 + t) = -200 - t$\n",
            "Since $t$ must be non-negative, we have $-200 - t \\geq 0$, so $t \\leq -200$. This is not possible.\n",
            "\n",
            "Let's re-interpret the problem.\n",
            "James collects 50 tins on \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "A jar of jellybeans has 14 blue jellybeans, 26 purple jellybeans and 40 orange jellybeans. If there are 200 jellybeans in the jar, how many are there of the red color? \n",
            "Answer:\n",
            "120 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $B$ be the number of blue jellybeans, $P$ be the number of purple jellybeans, and $O$ be the number of orange jellybeans.\n",
            "We are given that $B = 14$, $P = 26$, and $O = 40$.\n",
            "The total number of jellybeans is $B + P + O = 14 + 26 + 40 = 80$.\n",
            "We are told that there are 200 jellybeans in the jar.\n",
            "Let $R$ be the number of red jellybeans.\n",
            "The total number of jellybeans is the sum of the number of blue, purple, orange, and red jellybeans.\n",
            "So, $B + P + O + R = 200$.\n",
            "Substituting the given values, we have $14 + 26 + 40 + R = 200$.\n",
            "$80 + R = 200$.\n",
            "$R = 200 - 80$.\n",
            "$R = 120$.\n",
            "Therefore, there are 120 red jellybeans in the jar.\n",
            "\n",
            "<SOLUTION>120 \n",
            "Extracted:\n",
            "120\n",
            "******************** Question:\n",
            "Five adults and two children go to see a movie and buy $12 worth of concessions. The total cost of their trip is $76. If each child's ticket is $7, how much, in dollars, are the adult tickets? \n",
            "Answer:\n",
            "10 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Let $a$ be the number of adult tickets and $c$ be the number of children's tickets.\n",
            "We are given that there are five adults and two children, so $a+c = 5+2 = 7$.\n",
            "We are also given that the total cost of the trip is $12 worth of concessions, and the total cost is $76. So, $7a + 7c = 76$.\n",
            "We have a system of two linear equations with two variables:\n",
            "\\begin{align*} \\label{eq:1} a+c &= 7 \\\\ 7a+7c &= 76\\end{align*}\n",
            "From the first equation, we can write $c = 7-a$. Substitute this into the second equation:\n",
            "$$7a + 7(7-a) = 76$$\n",
            "$$7a + 49 - 7a = 76$$\n",
            "$$49 = 76$$\n",
            "This equation is inconsistent, so we must have made a mistake.\n",
            "Let's try another approach. Let $x$ be the price of an adult ticket and $y$ be the price of a child's ticket.\n",
            "We are given that there are five adults and two children, so $a$ is the number of adult tickets and $c$ is the number of children's tickets.\n",
            "We are given that there are 5 adults and 2 children, so $a+c = 5+2 = 7$.\n",
            "We are given that the total cost is $12 worth of concessions.\n",
            "The cost of the adult tickets is $ax$ and the cost of the children's tickets is $cy$.\n",
            "The total cost is $ax + cy = 12$.\n",
            "We are given that each child's ticket is $7, so $y = 7$.\n",
            "Thus, $ax + 7c = 12$.\n",
            "Since $a+c = 7$, we can write $c = 7-a$. Substitute this into the equation:\n",
            "$$ax + 7(7-a) = 12$$\n",
            "$$ax + 49 - 7a = 12$$\n",
            "$$ax - 7a = 12 - 49$$\n",
            "$$a(x-7) = -37$$\n",
            "$$a = \\frac{-37}{x-7}$$\n",
            "Since $a$ and $c$ must be integers, $x-7$ must be a factor of 37. Since 37 is a prime number, $x-7$ can be $1$ or $37$.\n",
            "If $x-7 = 1$, then $x = 8$. Then $a = \\frac{-37}{1} = -37$, which is not possible.\n",
            "If $x-7 = 37$, then $x = 44$. Then $a = \\frac{-37}{37} = -1$, which is not possible.\n",
            "Let's try a different approach.\n",
            "The total cost of the trip is $12. The number of adults is $a$ and the number of children is $c$.\n",
            "$$ax + 7c = 12$$\n",
            "We are given that there are 5 adults and 2 children, so $a+c = 5+2 = 7$.\n",
            "Substituting $c = 7-a$ into the equation, we get\n",
            "$$ax + 7(7-a) = 12$$\n",
            "$$ax + 49 \n",
            "Extracted:\n",
            "None\n",
            "******************** Question:\n",
            "Janelle had 26 green marbles. Then she bought 6 bags of blue marbles. There were 10 marbles in each bag.  She created a gift of 6 green marbles and 8 blue marbles and gave it to a friend. How many marbles does Janelle have now? \n",
            "Answer:\n",
            "72 \n",
            "Response:\n",
            "<start_working_out>\n",
            "Janelle starts with 26 green marbles.\n",
            "She buys 6 bags of blue marbles, with 10 marbles in each bag. So she buys 6 * 10 = 60 marbles of blue.\n",
            "Now she has 26 + 60 = 86 marbles.\n",
            "She creates a gift of 6 green marbles and 8 blue marbles.\n",
            "So she has 6 green marbles + 8 blue marbles = 14 marbles.\n",
            "After creating the gift, she has 86 - 14 = 72 marbles.\n",
            "<end_working_out>\n",
            "Janelle has 72 marbles now.\n",
            " \n",
            "Extracted:\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtcz_lpbVC92",
        "outputId": "bc1e6dbc-4e4e-4382-f7aa-1f56e716f044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_working_out>\n",
            "The square root of 101 is approximately 10.0498756.\n",
            "<SOLUTION>\n",
            "10.0498756<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjXGTkp7YNtB",
        "outputId": "7dbe3643-d756-4c32-acaf-2d5fe76f52ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gemma-3/tokenizer_config.json',\n",
              " 'gemma-3/special_tokens_map.json',\n",
              " 'gemma-3/tokenizer.model',\n",
              " 'gemma-3/added_tokens.json',\n",
              " 'gemma-3/tokenizer.json')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrqfyaRaXRL"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-B07m_HC5i7"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JMDDS0bC7jT"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX3xMj8hC6e0"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CfqZW5rC9zv"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG6-dP0JC-G2"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F2GVrPrDArH"
      },
      "source": [
        "Now, use the `gemma-3-finetune.gguf` file or `gemma-3-finetune-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "039e6d46575541878fc3b85980fbf6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a59b55dedcfe490bb5d873fe4c353743",
              "IPY_MODEL_d2bbfe42cd454037a21f89b97c2a0618",
              "IPY_MODEL_10fff27d605a46cc9647acfb535ff5d0"
            ],
            "layout": "IPY_MODEL_5faf0203e93d4e9382b0ea69daa80701"
          }
        },
        "a59b55dedcfe490bb5d873fe4c353743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a637d247e1e49e093e05243e66946ec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7c9ac282e39c43ccbadfb7604ff995c6",
            "value": "Map:‚Äá100%"
          }
        },
        "d2bbfe42cd454037a21f89b97c2a0618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c664cc7740d74f63a0a6a3d8940c8517",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_428bd52d92384da49398d3ae9b9fd9c2",
            "value": 1000
          }
        },
        "10fff27d605a46cc9647acfb535ff5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6eeefba4acb446c80508aa667be137f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23a3e05814f24a61933785fe3ceca3c8",
            "value": "‚Äá1000/1000‚Äá[00:00&lt;00:00,‚Äá9606.04‚Äáexamples/s]"
          }
        },
        "5faf0203e93d4e9382b0ea69daa80701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a637d247e1e49e093e05243e66946ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9ac282e39c43ccbadfb7604ff995c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c664cc7740d74f63a0a6a3d8940c8517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428bd52d92384da49398d3ae9b9fd9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6eeefba4acb446c80508aa667be137f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a3e05814f24a61933785fe3ceca3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}